# -*- coding: utf-8 -*-
"""Predictions of Stock Market Fluctuations Through Sentiment Analysis of the News.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X_b0ote0uTv-2m3LrKIaz8k33p_w0tjT

# **Data of top news headlines is feed to a Model which then predicts if the Dow Jones Index will go up or down.**

The Data is obtained from:

*   https://query1.finance.yahoo.com/v7/finance/download/%5EDJI?      
period1=1571798590&period2=1603420990&interval=1d&events=history&includeAdjustedClose=true 

  **==> This is the Stock Market Data from Yahoo Finance (DJI.CSV)** 



*  https://www.kaggle.com/aaron7sun/stocknews 

    **==> This is the News Dataset from Kaggle (Combined_News_DJIA.csv)     (thanks to aaron7)**
"""

pip install vaderSentiment

#Import Libraries ...
import pandas as pd
import numpy as np
from textblob import TextBlob
import re
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

#Load the data ...
from google.colab import files
files.upload()

#Store the Datasets ...
df1 = pd.read_csv('Combined_News_DJIA.csv')
df2 = pd.read_csv('DJI.csv')

#Display df1 ...
df1.head(5)

#Display df2 ...
df2.head(5)

df1.shape

df2.shape

#Merge the two datasets based on date ...

merged_df = df1.merge(df2, how='inner', on='Date', left_index= True)
merged_df

#Combine the headlines ...
com_headlines = []
for row in range(0, len(merged_df.index)):
  com_headlines.append(" ".join(str(x) for x in merged_df.iloc[row, 2:27]))
com_headlines[0]

#Clean Data => removing all the strange characters ....

clean_headlines = []

for i in range(0, len(com_headlines)):
  clean_headlines.append(re.sub("b[(')]", '', com_headlines[i] ))
  clean_headlines[i] = re.sub('b[(")]', '', clean_headlines[i])
  clean_headlines[i] = re.sub('b[(\)]', '', clean_headlines[i])
  clean_headlines[i] = re.sub('b[(")]', '', clean_headlines[i])
  clean_headlines[i] = re.sub("\'", '', clean_headlines[i])
  clean_headlines[i] = re.sub('1 ', '', clean_headlines[i])
  clean_headlines[i] = re.sub('0 ', '', clean_headlines[i])
clean_headlines[0]

"""***Looks Pretty Good!***"""

# Add the clean data to the dataframe ...
merged_df["Combined News"] = clean_headlines
merged_df

# Get the subjectivity of the Combined News Column ...
def getSubjectivity(text):
  return TextBlob(text).sentiment.subjectivity

# Get the polarity of the Combined News Column  ...
def getPolarity(text):
  return TextBlob(text).sentiment.polarity
getSubjectivity(merged_df["Combined News"][1]) # Display Subjectivity of the Text range [0 to 1] Zero being Very Objective and One Very Subjective
getPolarity(merged_df["Combined News"][1])     # Display Polarity of the Text ... Positive, Negative or Neutral range [ -1 to 1]

#Apply the Formulas to the Combined News Column ... And create two new columns
merged_df['Subjectivity'] = merged_df['Combined News'].apply(getSubjectivity)
merged_df['Polarity'] = merged_df['Combined News'].apply(getPolarity)

#Display the Dataset ...
merged_df.head()

#Get the Sentiment Intensity Analyzer ...

def getSen(text):
  sia = SentimentIntensityAnalyzer()
  sentiment = sia.polarity_scores(text)
  return sentiment

#Gather all the sentiment breakdown !!!!! .....
compound = []
neg = []
pos = []
neu = []
SIA = 0

for i in range(0, len(merged_df['Combined News'])):
  SIA = getSen(merged_df['Combined News'][i])
  compound.append(SIA['compound'])
  neg.append(SIA['neg'])
  neu.append(SIA['neu'])
  pos.append(SIA['pos'])

merged_df["Compound"] = compound
merged_df["Negative"] = neg
merged_df["Positive"] = pos
merged_df["Neutral"] = neu

merged_df

#create a list of column to keep

merged_df['Difference'] = merged_df['Adj Close'] - merged_df['Open'] # to observe the UPs and DOWNs of the stock
keep_columns = ['Label', 'Open', 'High', 'Low', 'Volume','Adj Close', 'Subjectivity', 'Polarity', 'Compound', 'Negative', 'Positive', 'Neutral']

df = merged_df[keep_columns]
df

#Create the feature data set

X= df
X = np.array(X.drop(['Label'], 1))

#Create the target data set
y = np.array(df['Label'])

#Split the data into 80% training and 20% testing

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

#Create and train the model
model = LinearDiscriminantAnalysis().fit(x_train, y_train)

#Show the models predictions
predictions = model.predict(x_test)
predictions

y_test # What is in y_test ????

# Compare the predictions and the actual data ...
com = predictions == y_test
trues = np.count_nonzero(com != 0)
total_percent_true = (trues/len(com)) * 100 # Percentage of correctness
total_percent_true # Pretty Accurate !!!!!!!!! ******

"""# ***Built by Ali Ali ...***
*This Sentiment Anaylsis was originally featured on the Computer Science Youtube Channel. *** * 

*Thanks very Much for the Informative Videos ****
"""